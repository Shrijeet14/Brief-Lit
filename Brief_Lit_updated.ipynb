{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFhQdENv1SlS"
   },
   "source": [
    "# ðŸ”¥ðŸ”¥ Let's Make PubMed abstracts easier to read !! with : \" Brief-Lit \" ðŸ“„ ðŸ”¥ðŸ”¥\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_28SgPEn1dSm"
   },
   "source": [
    "Confirming GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kW1dYF7G1OOc",
    "outputId": "63fe3d4f-4f47-4d9f-8aec-796b5326cb17"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pymLPDHhHoQj"
   },
   "source": [
    "## All Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fgb4iT4mHnaG",
    "outputId": "1135a385-bc07-4704-f9f5-6d5485b28c41"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow tensorflow-hub\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf # Importing tensorflow after reinstalling\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score  , accuracy_score\n",
    "import tensorflow_hub as hub # Importing tensorflow_hub after reinstalling\n",
    "import tf_keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tmcYf4_154Z"
   },
   "source": [
    "## Getting the Data\n",
    "\n",
    "**PubMed 200k RCT dataset**\n",
    "\n",
    "The PubMed 200k RCT dataset is described in Franck Dernoncourt, Ji Young Lee. PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts. International Joint Conference on Natural Language Processing (IJCNLP). 2017.\n",
    "\n",
    "**[Abstract:](https://)**\n",
    "\n",
    "PubMed 200k RCT is new dataset based on PubMed for sequential sentence classification. The dataset consists of approximately 200,000 abstracts of randomized controlled trials, totaling 2.3 million sentences. Each sentence of each abstract is labeled with their role in the abstract using one of the following classes: background, objective, method, result, or conclusion. The purpose of releasing this dataset is twofold. First, the majority of datasets for sequential short-text classification (i.e., classification of short texts that appear in sequences) are small: we hope that releasing a new large dataset will help develop more accurate algorithms for this task. Second, from an application perspective, researchers need better tools to efficiently skim through the literature. Automatically classifying each sentence in an abstract would help researchers read abstracts more efficiently, especially in fields where abstracts may be long, such as the medical field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RB4qFMih1OLz",
    "outputId": "28ba62c5-2b60-4632-f51d-4a679e71ef8f"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QVf_QUA11OJD",
    "outputId": "b53bf013-f3fc-4f9e-8746-046c000dbc57"
   },
   "outputs": [],
   "source": [
    "!ls pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6FgBABtk1OGx",
    "outputId": "c1cd8f98-1e54-4185-97b5-b1ee85c98dfa"
   },
   "outputs": [],
   "source": [
    "data_dir = \"/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign\"\n",
    "filenames = [data_dir + filename for filename in os.listdir(data_dir)]\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cCsGNVLIzwx"
   },
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4t-ddy5f1OD1"
   },
   "outputs": [],
   "source": [
    "# Reading The Lines  Of The Document\n",
    "\n",
    "def get_lines(filename):\n",
    "  with open(filename , \"r\") as f :\n",
    "    return f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eisc5iWQ1OBD",
    "outputId": "8b282489-7fd7-4e92-e591-ca53a47094a6"
   },
   "outputs": [],
   "source": [
    "train_lines = get_lines(data_dir+\"/train.txt\")\n",
    "train_lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BxwBFFJa1N-b",
    "outputId": "21e44364-0177-4359-b165-451100d567d7"
   },
   "outputs": [],
   "source": [
    "len(train_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onVpRFGh1N8B"
   },
   "outputs": [],
   "source": [
    "def preprocess_text_with_line_numbers(filename):\n",
    "  input_lines = get_lines(filename)\n",
    "  abstract_lines = \"\"\n",
    "  abstract_samples = []\n",
    "\n",
    "  for line in input_lines:\n",
    "    if line.startswith(\"###\"):\n",
    "      abstract_id = line\n",
    "      abstract_lines = \"\"\n",
    "    elif line.isspace():\n",
    "      abstract_line_split = abstract_lines.splitlines()\n",
    "\n",
    "\n",
    "      for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
    "        line_data = {}\n",
    "        target_text_split = abstract_line.split(\"\\t\")\n",
    "        line_data[\"target\"] = target_text_split[0]\n",
    "        line_data[\"text\"] = target_text_split[1].lower()\n",
    "        line_data[\"line_number\"] = abstract_line_number\n",
    "        line_data[\"total_lines\"] = len(abstract_line_split) - 1\n",
    "        abstract_samples.append(line_data)\n",
    "    else:\n",
    "      abstract_lines += line\n",
    "  return abstract_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lb-6GoJD1N51"
   },
   "outputs": [],
   "source": [
    "train_samples = preprocess_text_with_line_numbers(data_dir + \"/train.txt\")\n",
    "val_samples = preprocess_text_with_line_numbers(data_dir + \"/dev.txt\")\n",
    "test_samples = preprocess_text_with_line_numbers(data_dir + \"/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8lhT_J351N3b",
    "outputId": "88a8be34-f933-44d7-81e0-62d0533e616a"
   },
   "outputs": [],
   "source": [
    "print(\"Length Of Training Samples : \" , len(train_samples))\n",
    "print(\"Length Of Validation Samples : \" , len(val_samples))\n",
    "print(\"Length Of Test Samples : \" , len(test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8nSRZDRz1N1J",
    "outputId": "c770b869-284e-47e6-d5f5-705580a47659"
   },
   "outputs": [],
   "source": [
    "train_samples[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "GDfRqOSb1NyN",
    "outputId": "cec07627-0389-4b95-e616-0b7f2a542912"
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_samples)\n",
    "val_df = pd.DataFrame(val_samples)\n",
    "test_df = pd.DataFrame(test_samples)\n",
    "train_df.head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "oAW6FdWp1Nv5",
    "outputId": "e684dda0-11fa-4462-eaa6-572da32c951b"
   },
   "outputs": [],
   "source": [
    "train_df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "id": "IsBmi__Q1NtV",
    "outputId": "b9991cf6-4406-4152-cf02-330d0aa1f1ff"
   },
   "outputs": [],
   "source": [
    "# Let's check the length of Different lines\n",
    "train_df[\"total_lines\"].plot.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kn_Cgxdd1Nqh",
    "outputId": "df575619-8f97-46d5-b68d-81dbbd5e5fe4"
   },
   "outputs": [],
   "source": [
    "train_sentences = train_df[\"text\"].tolist()\n",
    "test_sentences = test_df[\"text\"].tolist()\n",
    "val_sentences = val_df[\"text\"].tolist()\n",
    "\n",
    "len(train_sentences) , len(test_sentences) , len(val_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FROyG6EMBvTN"
   },
   "source": [
    "Transforming Numeric Labels :\n",
    "\n",
    "One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mc42465b8Lsm"
   },
   "outputs": [],
   "source": [
    "ohe_encoder = OneHotEncoder(sparse_output=False)\n",
    "train_labels_one_hot = ohe_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1 , 1))\n",
    "test_labels_one_hot = ohe_encoder.transform(test_df[\"target\"].to_numpy().reshape(-1 , 1))\n",
    "val_labels_one_hot = ohe_encoder.transform(val_df[\"target\"].to_numpy().reshape(-1 , 1))\n",
    "\n",
    "# Note : The fit_transform method of OneHotEncoder expects a 2D array as input. Reshaping the target column from a 1D array (single column) to a 2D array (with one row and many columns) allows the encoder to function properly. This is because it treats each sample as a row and each category as a column when creating the one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qol7FF83JuhF"
   },
   "source": [
    "Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eaXKsdc3FdId",
    "outputId": "34cfa829-caca-446d-c5de-72d6e67afec4"
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoder = label_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1 , 1))\n",
    "test_labels_encoder = label_encoder.transform(test_df[\"target\"].to_numpy().reshape(-1 , 1))\n",
    "val_labels_encoder = label_encoder.transform(val_df[\"target\"].to_numpy().reshape(-1 , 1))\n",
    "train_labels_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ba2b4eFuKTZp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSj43h0aGkzR"
   },
   "outputs": [],
   "source": [
    "def evaluation_metrics_tf(model , test_pred , test_labels) :\n",
    "    accuracy = tf.keras.metrics.Accuracy()\n",
    "    precision = tf.keras.metrics.Precision()\n",
    "    recall = tf.keras.metrics.Recall()\n",
    "\n",
    "    # Updating state\n",
    "    accuracy.update_state(test_pred , test_labels)\n",
    "    precision.update_state(test_pred , test_labels)\n",
    "    recall.update_state(test_pred , test_labels)\n",
    "\n",
    "    # Evaluate Scores\n",
    "    accuracy_score = accuracy.result().numpy()\n",
    "    precision_score = precision.result().numpy()\n",
    "    recall_score = recall.result().numpy()\n",
    "    f1_score = 2 * (precision_score * recall_score) / (precision_score + recall_score + 1e-7)\n",
    "    confusion_matrix_tf = tf.math.confusion_matrix(test_labels , test_pred)\n",
    "\n",
    "    # Results Disctionary\n",
    "    results = {\n",
    "        \"Accuracy\" : accuracy_score ,\n",
    "        \"Recall\" : recall_score ,\n",
    "        \"Precision\" : precision_score ,\n",
    "        \"F1-Score\" : f1_score ,\n",
    "        \"Confusion Matrix\" : confusion_matrix_tf\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIOfFYEgHFR9"
   },
   "source": [
    "## Experimentation Of Different Models :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1Qj57ZTHQ6f"
   },
   "source": [
    "### Model 0 : Getting a baseline model : \" Machine Learing Based => Multinomial NB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "id": "dKgPqkko8LkW",
    "outputId": "d89b951f-6399-44d2-fe02-5a7d3af9c94f"
   },
   "outputs": [],
   "source": [
    "model_0 = Pipeline([\n",
    "    (\"tfid\", TfidfVectorizer()),\n",
    "    (\"clf\", MultinomialNB())\n",
    "]\n",
    ")\n",
    "\n",
    "\n",
    "model_0.fit(train_sentences , train_labels_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V08eZsS68LgU",
    "outputId": "28bd1e68-2c69-426a-fba4-69ba7a0d80b2"
   },
   "outputs": [],
   "source": [
    "model_0.score(X= val_sentences ,\n",
    "                 y= val_labels_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U4kDedzB8LXf",
    "outputId": "383d9ad1-cde1-49f8-f881-241b41986419"
   },
   "outputs": [],
   "source": [
    "model_0_preds = model_0.predict(val_sentences)\n",
    "model_0_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PX0ljfWVH1qF"
   },
   "outputs": [],
   "source": [
    "def evaluation_metrics_sklearn(model , test_pred , test_labels) :\n",
    "    accuracy = accuracy_score(test_labels , test_pred)\n",
    "    precision = precision_score(test_labels, test_pred, average='weighted')\n",
    "    recall = recall_score(test_labels , test_pred, average='weighted')\n",
    "    f1= f1_score(test_labels , test_pred, average='weighted')\n",
    "    confusion = confusion_matrix(test_labels , test_pred)\n",
    "    results = {\n",
    "        \"Accuracy\" : accuracy ,\n",
    "        \"Recall\" : recall ,\n",
    "        \"Precision\" : precision ,\n",
    "        \"F1-Score\" : f1 ,\n",
    "        \"Confusion Matrix\" : confusion\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BlgIMuxDH4vJ",
    "outputId": "74e38fdb-ab2e-4d27-9246-ec829c223cff"
   },
   "outputs": [],
   "source": [
    "model_0_results = evaluation_metrics_sklearn(test_labels= val_labels_encoder ,\n",
    "                                    test_pred= model_0_preds , model=model_0)\n",
    "model_0_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqjIF6UW8KzB"
   },
   "outputs": [],
   "source": [
    "# from helper_functions import calculate_results\n",
    "# model_0_results = calculate_results(y_true= val_labels_encoder ,\n",
    "#                                     y_pred= model_0_preds)\n",
    "# model_0_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RnVp4fHs8KvQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fm-owt26Q6Lc"
   },
   "source": [
    "### Model 1 : \" Conv 1D  \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1dio9pIR8KsA",
    "outputId": "d31e7185-9036-407c-c763-9f7a29901ea4"
   },
   "outputs": [],
   "source": [
    "# Lengths of Each Sentences\n",
    "sentence_len_list = [len(sentence.split()) for sentence in train_sentences]\n",
    "max_len = int(np.max(sentence_len_list))\n",
    "min_len = int(np.min(sentence_len_list))\n",
    "avg_len = int(np.mean(sentence_len_list))\n",
    "_95_percentile_len = int(np.percentile(sentence_len_list , 95))\n",
    "avg_len , max_len , min_len , _95_percentile_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "id": "bPT7WO8t8KpZ",
    "outputId": "c244a3d0-5874-4f78-b782-87694ec41f69"
   },
   "outputs": [],
   "source": [
    "# Distribution of the lengths of the sentences\n",
    "plt.hist(sentence_len_list , bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kmDe_mMs8Km-",
    "outputId": "40c0294c-131d-4da0-a0bb-08c7f7405c3e"
   },
   "outputs": [],
   "source": [
    "# Vocab length\n",
    "sentences_list = [sentence.split() for sentence in train_sentences]\n",
    "word_list = list(itertools.chain.from_iterable(sentences_list))\n",
    "vocab_len = len(set(word_list))\n",
    "vocab_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dU2NmuFfnwZU"
   },
   "source": [
    "#### Creating the Text Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ShBK7I8k8KjU"
   },
   "outputs": [],
   "source": [
    "text_vectorizer = TextVectorization(max_tokens = vocab_len ,\n",
    "                                    output_sequence_length = _95_percentile_len)\n",
    "\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LyqaFRlp8Kgb",
    "outputId": "63cdc36e-e732-479f-b625-f16e35e1c3de"
   },
   "outputs": [],
   "source": [
    "vocab = text_vectorizer.get_vocabulary()\n",
    "print(f\"Length of vocab : {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mkDnsMLn8Kdi",
    "outputId": "9a1c2768-6e4c-475b-8e6c-ea61e9e6099c"
   },
   "outputs": [],
   "source": [
    "text_vectorizer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KaLE6Ho_n6DA"
   },
   "source": [
    " #### Creating embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rwbycTx8Kaq"
   },
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(input_dim=len(vocab),\n",
    "                                   output_dim = 512 ,\n",
    "                                   mask_zero = True ,\n",
    "                                   name = \"embedding_layer\",\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_xOB1HV58KXS",
    "outputId": "5f61021f-58db-4bec-a615-6206ae61e08c"
   },
   "outputs": [],
   "source": [
    "# Checking a sample Vectorized and embedded sentence\n",
    "\n",
    "sample_sentence = train_sentences[np.random.randint(0 , len(train_sentences))]\n",
    "sample_sentence_vectorized = text_vectorizer([sample_sentence])\n",
    "sample_sentence_embedded = embedding_layer(sample_sentence_vectorized)\n",
    "print(f\"Sample Sentence : {sample_sentence}\")\n",
    "print(f\"Sample Sentence Vectorized : {sample_sentence_vectorized}\")\n",
    "print(f\"Sample Sentence Embedded : {sample_sentence_embedded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HgVI88hP8KRj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IuNlE0IkhUW"
   },
   "source": [
    "Turning Our Datasets  Into TensorFlow Datasets:  \"TensorFlow tf.data API\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvQnUIk-8KOg"
   },
   "outputs": [],
   "source": [
    "train_tf_dataset = tf.data.Dataset.from_tensor_slices((train_sentences , train_labels_one_hot))\n",
    "val_tf_dataset = tf.data.Dataset.from_tensor_slices((val_sentences , val_labels_one_hot))\n",
    "test_tf_dataset = tf.data.Dataset.from_tensor_slices((test_sentences , test_labels_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bNenVZKJ8KKc"
   },
   "outputs": [],
   "source": [
    "# Prefected Datasets\n",
    "train_tf_dataset= train_tf_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_tf_dataset = val_tf_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_tf_dataset = test_tf_dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gkgR3sb7kPtr",
    "outputId": "3f4572ef-95b4-417d-b298-694706539e47"
   },
   "outputs": [],
   "source": [
    "train_tf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gk0Esy-pkQv_",
    "outputId": "ef75bd0f-111b-4680-a286-968209dd63e8"
   },
   "outputs": [],
   "source": [
    "# Model Building\n",
    "inputs = layers.Input(shape=(1,) , dtype=tf.string)\n",
    "vectorized_text = text_vectorizer(inputs)\n",
    "embedding_text = embedding_layer(vectorized_text)\n",
    "x = layers.Conv1D(128 , kernel_size = 5 , padding=\"same\" , activation =\"relu\")(embedding_text)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(train_df['target'].nunique() , activation=\"softmax\")(x)\n",
    "model_1 = tf.keras.Model(inputs , outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZ-BkC_KkWLF"
   },
   "outputs": [],
   "source": [
    "# Compile Model\n",
    "model_1.compile(loss=\"categorical_crossentropy\" ,\n",
    "                optimizer = tf.keras.optimizers.Adam() ,\n",
    "                metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "FgpAODOskWJG",
    "outputId": "3fe62dfb-59c1-46d5-a3d3-cadbdfe8537b"
   },
   "outputs": [],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4pg9nT_4kWGI",
    "outputId": "a5af9aef-3727-4b39-b15b-43f908b50b57"
   },
   "outputs": [],
   "source": [
    "# Fitting the model_1\n",
    "model_1_history = model_1.fit(train_tf_dataset ,steps_per_epoch=int(0.1*len(train_tf_dataset)) ,  epochs = 5 , validation_data = val_tf_dataset , validation_steps = int(0.1*len(val_tf_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1IXNTUIakTo1",
    "outputId": "974a452d-357a-44e7-f6c2-5d4a0d3146e0"
   },
   "outputs": [],
   "source": [
    "model_1.evaluate(val_tf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U6ZydbwUkTlZ",
    "outputId": "bf12ec73-33dc-46b9-c80d-495b0e381579"
   },
   "outputs": [],
   "source": [
    "model_1_pred_probs = model_1.predict(val_tf_dataset)\n",
    "model_1_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cEIcHtImkTiy",
    "outputId": "af59d080-3c49-401d-c251-d4c21275785b"
   },
   "outputs": [],
   "source": [
    "model_1_preds = tf.argmax(model_1_pred_probs , axis=1)\n",
    "model_1_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ZH9TUoAkTgp",
    "outputId": "3d5a87cd-f52c-481d-b21c-55b950ab0da1"
   },
   "outputs": [],
   "source": [
    "model_1_results = evaluation_metrics_tf(model= model_1 ,test_labels=val_labels_encoder , test_pred=model_1_preds)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6S5a8i5WkTdy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PEEGGDgNSuU"
   },
   "source": [
    "### Model 2 : \" Using Pretrained Token Embeddings  \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DkwN-ixukSdw",
    "outputId": "50572ab5-40ca-4aa8-a4ff-fcbc386229d6"
   },
   "outputs": [],
   "source": [
    "!pip install -q sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PxoGdA7yZP4v"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7Qvq0vni_u7"
   },
   "source": [
    "##### Creating Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535,
     "referenced_widgets": [
      "01cc679c19664df29dad111d4c7251f2",
      "ea4f075b7b944b72bcc66d8cf3fbc2c1",
      "91b170255bb044b3aeda9beef61f51a6",
      "b2594439189041b1866ae8cd24231739",
      "8060df6904164ca49785eabbf4e081c3",
      "d8c90cf5e2064e9ebcd751d51697db47",
      "f68dcd44fce54c7aaa47b9ea2f2430ff",
      "0f46b5414a3c4a40adebfff40369063d",
      "31f9fc0162dc498a9bdf38b45e97e7ae",
      "eca37598bbab4e03ab6a2ab74cf32020",
      "74602c32def246fe90800e01a477dac0",
      "2fe444bf03194373bd856ced3df2c488",
      "00b1e2f74e75492fabfb9639152f04cc",
      "a11e2c9c4d624f95afebffb1b71d9c51",
      "f73e092337b74b75a63453cadf91c973",
      "010d89758173478da8a22b7511c77b57",
      "17d70bc000b942b7baf234258012d0a6",
      "c16ff9219ded40899713006d40610c31",
      "167b4c33e9d0480b90c39683a1a96014",
      "d7f0aa04612c41c49b159e74e3efc54b",
      "742ee8e7a73e4265b48e826679ab9ef7",
      "f94d50cc51fd4c358a6efde09943e658",
      "2fea89fce66a48ea9716f8de49790c7d",
      "ff3b68d4aaa942d7bdc4964d48816a87",
      "7742abcf98664605b96abde8ff837a7d",
      "f09a451e10f84faca976a685f37a9758",
      "6e3697cd91b843db9093e857a6771a40",
      "673cb549bdbc48ab91fcb3d10e87688f",
      "34ccb1375e8847229e4403f5d215f314",
      "cf78d6974db8454083eb31eb962ec78a",
      "dd55c3596011406daff5a8bf72ef838f",
      "95981e3746ee4ba19e42d7d2b05524c8",
      "d9ecade53a6a40d283c4bcc2cf90d162",
      "d4d7dc1e3c9141b29f1b726e92b4ff5d",
      "ddd1c4612c644ad7aeab473b4214f8b6",
      "7f19f1984f664b669156da2244493e26",
      "ed5231a1b3dc4d6fadaebb8e88b21fa2",
      "0f6ebaede8244d709e600abdd7add9b6",
      "95893e4e376545f99b353323a9c0410d",
      "bd50874d18d9469f87d39670880e813c",
      "4f2d7f09d9654b59a038cdd619d461f0",
      "06b6feaec15f4ba3b7e870b428a3720a",
      "f00f46b959984f278443be85c79253c9",
      "5d89ee84dc5246daa442808ae97877eb",
      "9ced7d7f010644a085b6ef3a320b3949",
      "bb7934fe90604fa79581ea32e8cb608a",
      "b9b947369cf6479290a0872fc5768c31",
      "49500d7915a6477983251fd8bd4e7505",
      "2797107e0bc948568ed83ccc1c5e6015",
      "4b5c29aed5b14c639a4444b6a4319b5d",
      "bb0d2bc568d2455aab4757b21efffcd4",
      "f3f3da8513cf4874ad63de6246cc6ccc",
      "11e097291e08422289f937879da3c317",
      "510fdab84e76478a84f5e3a930c1909c",
      "c49697e11e9f4a7194e3ab3b4b9473ca",
      "e0572cb195ef4882bd23df6f9d5bf980",
      "73c857767b544a2988f4ff4757da5f98",
      "08803472147a4cb9af0781e51556aa91",
      "c550cf0301884c26b7a91b014381652c",
      "e1447fb9d4e24aac904a983b1a2dce2c",
      "72e9978260f243f3994b36425b3812f1",
      "47217130b9864467a3b1f3a113fc55cb",
      "b6985156cfb44557b05b6d4c1ca5906d",
      "8a4549027c3341f388e88ee1eea7ccba",
      "cea795579e2447688074c6a676f89152",
      "7e1c305bb9b54f3cad8ee4e28896f7d9",
      "cb4e57bc4eaa4a0fa2f58020d84d01dd",
      "b898e1cfb46143958e72791b0da7d0d1",
      "5d5c6e8bd1be4d2aa452bd3cbffd6810",
      "afb4b5514509472c856ab5e13630f5ce",
      "ff24200b8e304cf58ba309bac5920893",
      "c76a6396938740e4af08d2f784c1e18e",
      "8077dbc706864da48ed6466acc9e674d",
      "71a12e9aa0054e149461ca8420685cf9",
      "d03fa90f1310491ca1c6816ff2010cc3",
      "eb607cc1b3f24715af953de7127d88e4",
      "757fd47ed778421a8c9b441bba7a8013",
      "e3119b6d66c34210b9680a2a8eced4a9",
      "ca64af51dafe4dd38c4e937d3a58c4d9",
      "8400a84d16bd4acebd29c36972953998",
      "72533b20168f408cb7052b3c78256e61",
      "0e4d91acc0664be1ab0736a956d8d11b",
      "0dbcd776beee45d786de8695a5e9f712",
      "85f817868e4c4830a9f37487bea65a46",
      "2fc63cca488849a0a0fd4a2b0e688549",
      "c6d00d7f795b4c8b8c81c3f21b8ca0b4",
      "9c19940988da41ea9b09301dc6baae63",
      "77227d57769844f9b18d1f6070bf6f0d",
      "88e5afeb7f9d476ca65178c6623e9627",
      "f2ebf7342e0c442baf32195b3db3699a",
      "25c65e968d4e42a497ea06292479eefb",
      "73cfcc11b4c746d29c2f2a739ae68c57",
      "b28a7d996ec645b0afcd4d684e7266af",
      "101c801e32de44efa5f005d355a1764a",
      "d0f9fb4d7af84aa78e09c09e15d58702",
      "8b35cd1cc57a4fb4ac43c25aa59acd3b",
      "ef2f367980bc4872b6321b9d7f792a3d",
      "810ce0631c2b4401a55d489f53517457",
      "e863453f3566427ba9a8852720fe3bff",
      "956a83a2d6a9479781c6f1383917041c",
      "538533e803c9424488cf892050d70920",
      "04b0000f2fb44a8fbb85968f1d234b8c",
      "7e3ab0da8835475eb04994ba82fab0bb",
      "d586e39762a84140ba7d86a06bdf0b91",
      "ae1493fc3b8c4380a041b25076d256db",
      "b886a0b3fb29407b8921b4267c174e4a",
      "74df804f967b4a389babdefc3957adaa",
      "4d9c96d8e8d54d47888b4569a1644e97",
      "3dd35760c51540ebb9145bf0994ef01b",
      "be784f15feb648cf944a111848f81855",
      "4dc0a6c4b4ba48f09bcd2660ccea5e8b",
      "0219053f9be34e21aee1f80e19e0e404",
      "0d476f0ff23d46c1aaf00c052c681e69",
      "3d50069538a946dabe5cac6939df6dbb",
      "48fa72691d224604b526c5966bf0170e",
      "852ae8a68bf84b9fbf41d7ea8278f684",
      "6dda27b4f5174b428da9d04508a2ba34",
      "431fd5e2a0f8493fa39a30936558fc81",
      "fdd5c76a2ed84f438b30838dbf8747e4",
      "1ff310674864486e975ac707bd919652",
      "d61b8fa7f41b465f99728efbbb1b1ca7"
     ]
    },
    "id": "B1E7__waZP0X",
    "outputId": "3e7b517e-d803-438d-f771-1128d4994d8f"
   },
   "outputs": [],
   "source": [
    "sentence_encoder = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElNfseXmjFke"
   },
   "source": [
    "##### Preparing our dataset (Embedding the  train , text , validation dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PWwSk1VNZPxj",
    "outputId": "16a8e8d3-a3a3-4383-aa9d-b07f24d0ffbc"
   },
   "outputs": [],
   "source": [
    "print(\"Encoding training sentences...\")\n",
    "train_embeddings = sentence_encoder.encode(train_sentences)\n",
    "print(\"Encoding validation sentences...\")\n",
    "val_embeddings = sentence_encoder.encode(val_sentences)\n",
    "print(\"Encoding test sentences...\")\n",
    "test_embeddings = sentence_encoder.encode(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4lx0MMN7ZPva",
    "outputId": "059598d0-7794-48c8-c949-6a8d17015bb1"
   },
   "outputs": [],
   "source": [
    "embedding_dim = train_embeddings.shape[1]  # Typically 384 for all-MiniLM-L6-v2\n",
    "print(f\"Embedding dimension: {embedding_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xEjmdwybZPsz"
   },
   "outputs": [],
   "source": [
    "# Convert to TensorFlow dataset\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_embeddings, train_labels_one_hot))\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((val_embeddings, val_labels_one_hot))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_embeddings, test_labels_one_hot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCCJvWeQkGtZ"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rl0cTKSQZPqw"
   },
   "outputs": [],
   "source": [
    "# Batch and prefetch\n",
    "batch_size = 32\n",
    "train_ds = train_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2m-10NwXZPo6"
   },
   "outputs": [],
   "source": [
    "# Build the model directly without using a function\n",
    "num_classes = train_df['target'].nunique()\n",
    "inputs = layers.Input(shape=(embedding_dim,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q3hQWL_0ZPll"
   },
   "outputs": [],
   "source": [
    "# Dense layers with ReLU activation\n",
    "x = layers.Dense(256, activation='relu')(inputs)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dropout(0.2)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNNcsNB2ZPiy"
   },
   "outputs": [],
   "source": [
    "# Output layer\n",
    "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model_2 = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y99NKdnvZrRY"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_2.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "id": "36LjyJphZrOU",
    "outputId": "88db6e46-2b23-4f9b-854f-67191d54b90e"
   },
   "outputs": [],
   "source": [
    "# Model summary\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "le3K2Am1ZrMN",
    "outputId": "3e0f5c4a-d424-4ceb-be02-3a58b9933b4b"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model_2_history = model_2.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AiHc3qFiZrKd",
    "outputId": "23c9b660-606d-4295-fc8b-8b910015077e"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model_2.evaluate(test_ds)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OyxgmMcQZrG4",
    "outputId": "4682360a-56a0-419a-c7ec-bdcbc35ab483"
   },
   "outputs": [],
   "source": [
    "model_2_pred_probs = model_2.predict(val_ds)\n",
    "model_2_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ivCKB9UZrDy",
    "outputId": "45b644cb-6a63-4b7a-ec0f-70730de0b2e6"
   },
   "outputs": [],
   "source": [
    "model_2_preds = tf.argmax(model_2_pred_probs , axis=1)\n",
    "model_2_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0KPu39HqZrAV",
    "outputId": "a2442270-fa28-4a85-f740-7b55c95f01cd"
   },
   "outputs": [],
   "source": [
    "model_2_results = evaluation_metrics_tf(model= model_2 ,test_labels=val_labels_encoder , test_pred=model_2_preds)\n",
    "model_2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h37V6aUFeM2y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2O-vL_vkkxI"
   },
   "source": [
    "### Model 3 : Conv 1D with \"Character Encoding\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2WCuTu4w20J"
   },
   "source": [
    "#### Creating character Level Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5K6HueD3eMz8"
   },
   "outputs": [],
   "source": [
    "def split_chars(text):\n",
    "  return \" \".join(list(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aaMr4n6-eMxY",
    "outputId": "8dab0a7d-93a7-4ea2-d2d9-99410624a2ec"
   },
   "outputs": [],
   "source": [
    "# splitting sequence-level data splits into chanracter-level data splits\n",
    "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
    "val_chars = [split_chars(sentence) for sentence in val_sentences]\n",
    "test_chars = [split_chars(sentence) for sentence in test_sentences]\n",
    "print(train_chars[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r96KuJnxeMuZ",
    "outputId": "27343a43-06d4-4ec6-b888-bffaad3a6ba4"
   },
   "outputs": [],
   "source": [
    "# What's the average character length?\n",
    "char_lens = [len(sentence) for sentence in train_sentences]\n",
    "mean_char_len = np.mean(char_lens)\n",
    "mean_char_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "utdiQbiMeMrn",
    "outputId": "c5957fd2-2764-4cfc-8655-22bcf5d100c4"
   },
   "outputs": [],
   "source": [
    "# Checking the distribution of our sequences at character-level\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(char_lens, bins=7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ToAs-SRkeMpp",
    "outputId": "88e3ed5b-3eb4-4927-966c-665901733aba"
   },
   "outputs": [],
   "source": [
    "# Find what character length covers 95% of sequences\n",
    "output_seq_char_len = int(np.percentile(char_lens, 95))\n",
    "output_seq_char_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "KKgOusiWeMni",
    "outputId": "2a8806ea-8865-4746-81e0-7e9c015eb518"
   },
   "outputs": [],
   "source": [
    "# Get all keyboard characters for char-level embedding\n",
    "import string\n",
    "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
    "alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ZCn3pfjeMjo"
   },
   "outputs": [],
   "source": [
    "# Create char-level token vectorizer instance\n",
    "NUM_CHAR_TOKENS = len(alphabet) + 2\n",
    "char_vectorizer = TextVectorization(max_tokens=NUM_CHAR_TOKENS,\n",
    "                                    output_sequence_length=output_seq_char_len,\n",
    "                                    standardize=\"lower_and_strip_punctuation\",\n",
    "                                    name=\"char_vectorizer\")\n",
    "\n",
    "# Adapt character vectorizer to training characters\n",
    "char_vectorizer.adapt(train_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "euqk2HuReMg8",
    "outputId": "d1189f9f-4295-48e7-dcf9-ab46097a2078"
   },
   "outputs": [],
   "source": [
    "# Check character vocabulary characteristics\n",
    "char_vocab = char_vectorizer.get_vocabulary()\n",
    "print(f\"Number of different characters in character vocab: {len(char_vocab)}\")\n",
    "print(f\"5 most common characters: {char_vocab[:5]}\")\n",
    "print(f\"5 least common characters: {char_vocab[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sybiZeJUeMbl"
   },
   "outputs": [],
   "source": [
    "# Create char embedding layer\n",
    "char_embed = layers.Embedding(input_dim=NUM_CHAR_TOKENS,\n",
    "                              output_dim=100,\n",
    "                              mask_zero=False,\n",
    "                              name=\"char_embed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UcaUZMCqpGSG",
    "outputId": "f13359f9-5107-42ee-8b52-0688bbf67716"
   },
   "outputs": [],
   "source": [
    "# Test out character embedding layer\n",
    "print(f\"Charified text (before vectorization and embedding):\\n{train_chars[30]}\\n\")\n",
    "char_embed_example = char_embed(char_vectorizer([train_chars[30]]))\n",
    "print(f\"Embedded chars (after vectorization and embedding):\\n{char_embed_example}\\n\")\n",
    "print(f\"Character embedding shape: {char_embed_example.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duuhVLmSw-sL"
   },
   "source": [
    "#### Building The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_4lnNk-pGOX"
   },
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(1,) , dtype = \"string\")\n",
    "char_vectors = char_vectorizer(inputs)\n",
    "char_embeddings = char_embed(char_vectors)\n",
    "x = layers.Conv1D(128 , kernel_size = 5 , padding=\"same\" , activation =\"relu\")(char_embeddings)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(train_df['target'].nunique() , activation=\"softmax\")(x)\n",
    "model_3 = tf.keras.Model(inputs , outputs , name = \"model_3_conv1d_char_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ep0uufoxpGKt"
   },
   "outputs": [],
   "source": [
    "model_3.compile(loss = \"categorical_crossentropy\" , optimizer = tf.keras.optimizers.Adam() , metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "id": "KqIZo0nUpGHQ",
    "outputId": "04490c87-80fd-4cc3-a575-b3be8d4785a6"
   },
   "outputs": [],
   "source": [
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8NBeWampGEE"
   },
   "outputs": [],
   "source": [
    "# Create char datasets\n",
    "train_char_dataset = tf.data.Dataset.from_tensor_slices((train_chars, train_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_char_dataset = tf.data.Dataset.from_tensor_slices((val_chars, val_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ty7PHSznpGBE",
    "outputId": "e4c06eeb-e7dc-4354-e37e-06e538195991"
   },
   "outputs": [],
   "source": [
    "model_3_history = model_3.fit(train_char_dataset,\n",
    "                              steps_per_epoch=int(0.1 * len(train_char_dataset)),\n",
    "                              epochs=10,\n",
    "                              validation_data=val_char_dataset,\n",
    "                              validation_steps=int(0.1 * len(val_char_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCHGr2VbxCya"
   },
   "source": [
    "#### Evaluating our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P9v57IpppF-E",
    "outputId": "de9187bf-af8a-4653-d51b-2ce2677002b3"
   },
   "outputs": [],
   "source": [
    "model_3.evaluate(val_char_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mQ3bnNSnpF6i",
    "outputId": "6781b6cb-e95e-46e4-ba54-e5ccc0a6089d"
   },
   "outputs": [],
   "source": [
    "model_3_pred_probs = model_3.predict(val_char_dataset)\n",
    "model_3_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EYrUT3gkpF3c",
    "outputId": "f97b8663-4a3b-4d4f-8df4-bb807bf0c4bc"
   },
   "outputs": [],
   "source": [
    "model_3_preds = tf.argmax(model_3_pred_probs, axis=1)\n",
    "model_3_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6L6yDhN2pF0N",
    "outputId": "06374338-5867-4709-ea32-971057e7ac1d"
   },
   "outputs": [],
   "source": [
    "model_3_results = evaluation_metrics_tf(model = model_3 , test_labels=val_labels_encoder,test_pred=model_3_preds)\n",
    "model_3_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "opFbjYPPpFwd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDnu4IUMxMke"
   },
   "source": [
    "### Model4 : Combining Pretrained Embeddings + Characters Embeddings (hybrid Embedding)\n",
    "- Create a token-level embedding model\n",
    "- Crate a character-level model\n",
    "- Building a series of output layers\n",
    "- construct a modle which takes character-level sequences as input and produces sequence label probabilities as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tqhn0os2e-kL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLsE-SWwTIqY"
   },
   "source": [
    "#### Create a token-level embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83EQZIa9wzba"
   },
   "outputs": [],
   "source": [
    "# # Define the custom Sentence Encoder Layer\n",
    "# class SentenceEncoderLayer(tf.keras.layers.Layer):\n",
    "#     def __init__(self, model_name='all-MiniLM-L6-v2', **kwargs):\n",
    "#         super(SentenceEncoderLayer, self).__init__(**kwargs)\n",
    "#         self.model_name = model_name\n",
    "#         self.encoder = SentenceTransformer(model_name)\n",
    "#         self.trainable = False  # Set to True if you want it to be trainable\n",
    "#         self.output_dim = self.encoder.get_sentence_embedding_dimension()\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         # This ensures the layer knows its output shape\n",
    "#         self.built = True\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         # Remove extra dimension if needed: inputs shape might be (batch_size, 1)\n",
    "#         inputs = tf.squeeze(inputs, axis=1)\n",
    "#         return tf.py_function(\n",
    "#             func=self._encode_sentences,\n",
    "#             inp=[inputs],\n",
    "#             Tout=tf.float32\n",
    "#         )\n",
    "\n",
    "#     def _encode_sentences(self, sentences):\n",
    "#         # Convert tensor to list of strings\n",
    "#         if isinstance(sentences, tf.Tensor):\n",
    "#             sentences = [s.decode('utf-8') for s in sentences.numpy()]\n",
    "\n",
    "#         # Get embeddings\n",
    "#         embeddings = self.encoder.encode(sentences)\n",
    "#         return tf.convert_to_tensor(embeddings, dtype=tf.float32)\n",
    "\n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         # Specify the output shape explicitly\n",
    "#         return (input_shape[0], self.output_dim)\n",
    "\n",
    "#     def get_config(self):\n",
    "#         config = {\n",
    "#             'model_name': self.model_name\n",
    "#         }\n",
    "#         base_config = super(SentenceEncoderLayer, self).get_config()\n",
    "#         return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aoK181g5wzYr"
   },
   "outputs": [],
   "source": [
    "# # Setup token input branch\n",
    "# token_inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"token_input_main\")\n",
    "# sentence_encoder_layer = SentenceEncoderLayer()\n",
    "# token_embeddings = sentence_encoder_layer(token_inputs)\n",
    "# # Ensure the shape is set explicitly after processing\n",
    "# # token_embeddings = tf.reshape(token_embeddings, [-1, sentence_encoder_layer.output_dim])\n",
    "# token_output = layers.Dense(128, activation=\"relu\")(token_embeddings)\n",
    "# token_model = tf.keras.Model(inputs=token_inputs,\n",
    "#                             outputs=token_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1vwKUFEwzV9"
   },
   "outputs": [],
   "source": [
    "# token_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2w_70MCMTMgE"
   },
   "source": [
    "#### Crate a character-level model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P8KjCdpPwzTB"
   },
   "outputs": [],
   "source": [
    "# # 2. Setting up char inputs/model\n",
    "# char_inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"char_input\")\n",
    "# char_vectors = char_vectorizer(char_inputs)\n",
    "# char_embeddings = char_embed(char_vectors)\n",
    "# char_bi_lstm = layers.Bidirectional(layers.LSTM(25))(char_embeddings)\n",
    "# char_model = tf.keras.Model(inputs=char_inputs,\n",
    "#                             outputs=char_bi_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21d1ycn3wzQO"
   },
   "outputs": [],
   "source": [
    "# char_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BazB3oE2TSmi"
   },
   "source": [
    "#### Building a series of output layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fqopt8V6wzNc"
   },
   "outputs": [],
   "source": [
    "# # 3. Concatenating token and char inputs (create hybrid token embedding)\n",
    "# token_char_concat = layers.Concatenate(name=\"token_char_hybrid\")([token_model.output,\n",
    "#                                                                   char_model.output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUbYdgqcwzKz"
   },
   "outputs": [],
   "source": [
    "# # 4. Creating output layers - adding of dropout\n",
    "# combined_dropout = layers.Dropout(0.5)(token_char_concat)\n",
    "# combined_dense = layers.Dense(200, activation=\"relu\")(combined_dropout)\n",
    "# final_dropout = layers.Dropout(0.5)(combined_dense)\n",
    "# output_layer = layers.Dense(num_classes, activation=\"softmax\")(final_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VtLo8CqiwzH-"
   },
   "outputs": [],
   "source": [
    "# # 5. Constructing the model with char and token inputs\n",
    "# model_4 = tf.keras.Model(inputs=[token_model.input, char_model.input],\n",
    "#                          outputs=output_layer,\n",
    "#                          name=\"model_4_token_and_char_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ID_ZHbeVwzFJ"
   },
   "outputs": [],
   "source": [
    "# model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpZObbyFwzA-"
   },
   "outputs": [],
   "source": [
    "# # Plot and visualizations\n",
    "# from keras.utils import plot_model\n",
    "# plot_model(model_4 , show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1zJ0ObvVf3w"
   },
   "source": [
    "#### Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IBDqVEx0S3XL"
   },
   "outputs": [],
   "source": [
    "# # Combine Char + token model\n",
    "# model_4.compile(loss = \"categorical_crossentropy\",\n",
    "#                 optimizer=tf.keras.optimizers.Adam(),\n",
    "#                 metrics =[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JKbLNc3VlwU"
   },
   "source": [
    "#### Combining token and character data into a tf.data Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QJ61VoAsS3T2"
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Combine chars and tokens into a dataset\n",
    "# train_char_token_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars)) # make data\n",
    "# train_char_token_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot) # make labels\n",
    "# train_char_token_dataset = tf.data.Dataset.zip((train_char_token_data, train_char_token_labels)) # combine data and labels\n",
    "\n",
    "# # Prefetch and batch train data\n",
    "# train_char_token_dataset = train_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# # Repeat same steps validation data\n",
    "# val_char_token_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars))\n",
    "# val_char_token_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "# val_char_token_dataset = tf.data.Dataset.zip((val_char_token_data, val_char_token_labels))\n",
    "# val_char_token_dataset = val_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLwYUirKV-b_"
   },
   "source": [
    "#### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1VRtH9nS3Rp"
   },
   "outputs": [],
   "source": [
    "# model_4_history = model_4.fit(train_char_token_dataset, # train on dataset of token and characters\n",
    "#                               steps_per_epoch=int(0.1 * len(train_char_token_dataset)),\n",
    "#                               epochs=10,\n",
    "#                               validation_data=val_char_token_dataset,\n",
    "#                               validation_steps=int(0.1 * len(val_char_token_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kvbE23dyS3HF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DmRFdn-OS3EX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Dm_XnbEuDy4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJsOc8snuDwc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zuHsKqp2uDtH"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# For reproducibility (optional)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Define the number of classes for the final prediction (adjust as needed)\n",
    "num_classes = train_labels_one_hot.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "8c68946c9dab4a21bca6ef68686f6d89",
      "3dd236b452fa4c3c85f69aeb546a25b6",
      "65c836dc36af454a9c796e74eec2e2fe",
      "9d6c56d7b0fa48e285d181ee17a0e729",
      "c5a3204fe1904291a270a36f3223d582",
      "d051924358424bed8e7ba941db029e80",
      "5b8f13315db34861a9d693e8ec9ef5e9",
      "ad1904d0a9ad4e57a59cc156f64b0557",
      "f08283205c114a2e92abf62c4f325b3c",
      "556a074b36c34469981f12adc56a6b6a",
      "e096e1604b8c4a4bbad7774f68ae22e5",
      "c09fcf7aacab4b9480723d5b0554452a",
      "cb862cc3592b4ff2a77af2fbfcc95184",
      "761fc3887b9c42ec882b580e59b28133",
      "157dd5a074dc432f870e94390e101c3d",
      "5490ee37c7dc46d7ad94726763be1e9f",
      "cb43c96cc4df4f54b392c52db3e69543",
      "279184db4f42448e87c33c3d8022f818",
      "f46c675b1d9e47ae8a8a084f9b4f4f93",
      "d78ecc6a99a94c909a0ebd8ec69b8ff2",
      "06a49c7778b7402eb7682d4f9ef21577",
      "b81ede7cfad94f2ab4c3893cf1f0efdb"
     ]
    },
    "id": "Q7GapmKWuDoS",
    "outputId": "96da6354-2995-4f42-ce6b-f954e49809b6"
   },
   "outputs": [],
   "source": [
    "# Initialize the SentenceTransformer model\n",
    "sent_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "output_dim = sent_encoder.get_sentence_embedding_dimension()\n",
    "\n",
    "# Precompute embeddings for training and validation sentences\n",
    "train_sentence_embeddings = sent_encoder.encode(train_sentences, show_progress_bar=True)\n",
    "val_sentence_embeddings = sent_encoder.encode(val_sentences, show_progress_bar=True)\n",
    "\n",
    "# Convert the embeddings to float32 numpy arrays (or tensors later)\n",
    "train_sentence_embeddings = np.array(train_sentence_embeddings, dtype=np.float32)\n",
    "val_sentence_embeddings = np.array(val_sentence_embeddings, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0WOjFNduDlq"
   },
   "outputs": [],
   "source": [
    "# Define the numeric input for precomputed sentence embeddings\n",
    "token_input = layers.Input(shape=(output_dim,), dtype=tf.float32, name=\"token_input_main\")\n",
    "# Optionally, add a fully connected layer\n",
    "token_dense = layers.Dense(128, activation=\"relu\")(token_input)\n",
    "\n",
    "# Create a model for the token branch\n",
    "token_model = Model(inputs=token_input, outputs=token_dense, name=\"token_branch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RZ-Uwhv0SOZ"
   },
   "outputs": [],
   "source": [
    "# Define character input, assuming each example is a single string (shape=())\n",
    "char_input = layers.Input(shape=(), dtype=tf.string, name=\"char_input\")\n",
    "\n",
    "# Example: using a TextVectorization layer to tokenize the char input.\n",
    "# Adjust parameters as needed. Here we assume a maximum of 100 characters (you can adjust).\n",
    "max_chars = 100\n",
    "char_vectorizer = layers.TextVectorization(max_tokens=2000, output_mode='int', output_sequence_length=max_chars)\n",
    "# If you haven't adapted the vectorizer yet, do it on training data (convert input to a tf.data.Dataset of strings)\n",
    "char_vectorizer.adapt(train_chars)\n",
    "\n",
    "# Create an embedding layer for characters (vocab_size matches the vectorizer's output size)\n",
    "vocab_size = char_vectorizer.vocabulary_size()\n",
    "char_embed = layers.Embedding(input_dim=vocab_size, output_dim=64)\n",
    "\n",
    "# Process the char input through the vectorizer and embedding layers\n",
    "char_vectors = char_vectorizer(char_input)\n",
    "char_embeddings = char_embed(char_vectors)\n",
    "char_bi_lstm = layers.Bidirectional(layers.LSTM(25))(char_embeddings)\n",
    "\n",
    "# Build a model for the char branch\n",
    "char_model = Model(inputs=char_input, outputs=char_bi_lstm, name=\"char_branch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "id": "B8LAXhlo0R8G",
    "outputId": "55f7e7f9-f6e8-49f0-f07f-d9e978e0acdc"
   },
   "outputs": [],
   "source": [
    "# Concatenate the outputs of the token branch and the char branch\n",
    "combined = layers.Concatenate(name=\"token_char_hybrid\")([token_model.output, char_model.output])\n",
    "\n",
    "# Add dropout and dense layers as needed\n",
    "x = layers.Dropout(0.5)(combined)\n",
    "x = layers.Dense(200, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "output_layer = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# Construct the final multi-input model\n",
    "model_4 = Model(inputs=[token_model.input, char_model.input], outputs=output_layer, name=\"model_4_token_and_char_embeddings\")\n",
    "\n",
    "# Compile the model\n",
    "model_4.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "model_4.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eWQg07mM0Rsq"
   },
   "outputs": [],
   "source": [
    "# Build a tf.data.Dataset that provides the token embeddings and char inputs along with labels\n",
    "\n",
    "# For training:\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        \"token_input_main\": train_sentence_embeddings,  # numeric embeddings\n",
    "        \"char_input\": train_chars                        # raw text characters\n",
    "    },\n",
    "    train_labels_one_hot\n",
    "))\n",
    "train_dataset = train_dataset.batch(8).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# For validation:\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        \"token_input_main\": val_sentence_embeddings,\n",
    "        \"char_input\": val_chars\n",
    "    },\n",
    "    val_labels_one_hot\n",
    "))\n",
    "val_dataset = val_dataset.batch(8).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uH7iFQn-0RmE",
    "outputId": "5d48f5d7-501c-41a2-cfe4-f4bf1884d9b9"
   },
   "outputs": [],
   "source": [
    "# Train the model (adjust steps_per_epoch and validation_steps as appropriate)\n",
    "model_4_history = model_4.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    validation_data=val_dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yn1xPNiU0Rh3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GFeknTex0Rep"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "de38OmWb0RVU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWQrhIl75CHD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iwmtoIR5QsH"
   },
   "source": [
    "### Model 5: Transfer Learning with pretrained token embeddings + character embeddings + positional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "85K92dnQ5CDs",
    "outputId": "f62628a0-6e10-4219-b068-f575c59b0cef"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Inspect training dataframe\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0r0te9WV5CAe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zRN38RrO5B82"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ooBJrRjV5B4u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YbVP7NMZ5B1D"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HURKhP085Bvd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
